B = 10
lr  = 0.1
no momentum
no weight decay

########################

version_2 = iid with 1000
version_3 = non iid with 0.1

31 - local 1 iid
35 - local 20 iid
33 - local 1 non-iid
31 - local 20 non-iid


https://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@github.com/aerte/DFL.git
module load cuda/11.7 scipy/1.10.1-python-3.9.17 matplotlib/3.7.1-numpy-1.24.3-python-3.9.17
source ../torch_dl/bin/activate

########################

alpha = 0.1, 100 and 1000
100 and 1000 are approximately iid

/Users/faerte/Desktop/deep_learning/DFL

Implement and run:
More complex CNN model for 10 local epochs
20, 100 or more local epochs


Stuff to ask Bo:
- What about a repo link? Should we just use hers or our adapted one??
- It says to include a jupyter notebook, should we make it run on CPU? or just assume it runs on GPU?

Poster: 2 minutes per person + 1-2 minutes for questions
 Write about the dirichlet distribution




wandb.init(
        # set the wandb project where this run will be logged
        project="test-run-cifar_Version%01d" % conf.version,

        # track hyperparameters and run metadata
        config={
            "rounds": 5,
            "dataset": "CIFAR"
        }
    )


                wandb.log({'server_loss': tt_loss})
                wandb.log({'server_accuracy': tt_accu})


The code runs by running the train_eff for every client in every round, so code that records the
data has to evaluate the data and save it in a specific round

Save prediction in run_train
save server in

   #### Logging Data and recording labels + server prediction

                wandb.log({'server_loss': tt_loss})
                wandb.log({'server_accuracy': tt_accu})

                ####


The process runs with 2 gpus?? Can it run on 1?

https://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@github.com/aerte/DFL.git

if [ "$i" -lt "$num2" ]; then
                gpu_index=1
            elif [ "$i" -ge "$num2" ] && [ "$i" -lt "$num3" ]; then
                gpu_index=2
            fi
            echo "|GPU INDEX|CLIENT INDEX|${gpu_index}|${i}"
            export CUDA_VISIBLE_DEVICES="$gpu_index"

Plot the class distribution as a histogram for one class


class VGGModel(nn.Module):
    # Inspiration from  https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/
    def __init__(self, num_channel):
        super(CNNModel, self).__init__()
        num_feat = 64 * 5 * 5 if num_channel == 3 else 1024
        self.num_feat = num_feat
        self.layer = nn.Sequential(
            nn.Conv2d(num_channel, 32, kernel_size=5),
            nn.ReLU(True),
            nn.Conv2d(num_channel, 32, kernel_size=5,padding='same'),
            nn.MaxPool2d(2),
            nn.ReLU(True),
            nn.Conv2d(32, 64, kernel_size=5),
            nn.ReLU(True),
            nn.Conv2d(64, 64, kernel_size=5, padding='same'),
            nn.MaxPool2d(2),
            nn.ReLU(True))
        self.cls_layer = nn.Sequential(
            nn.Linear(num_feat, 512),
            nn.ReLU(True),
            nn.Linear(512, 10))

    def forward(self, x):
        feat = self.layer(x)
        out = self.cls_layer(feat.view(len(x), self.num_feat))
        return out

class VGG16(nn.Module):
    # Inspiration from https://blog.paperspace.com/vgg-from-scratch-pytorch/
    # VGG11 (D): https://arxiv.org/pdf/1409.1556.pdf
    def __init__(self, num_channel):
        super(VGG16, self).__init__()
        num_feat = 64 * 5 * 5 if num_channel == 3 else 1024
        self.num_feat = num_feat

        self.layer1 = nn.Sequential(
            nn.Conv2d(num_channel, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
        )
        self.layer2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.layer3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.layer4 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.layer5 = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Linear(4096, 10)
        )

    def forward(self, x):
        feat = self.layer1(x)
        feat = self.layer2(feat)
        feat = self.layer3(feat)
        feat = self.layer4(feat)
        feat = self.layer5(feat)
        feat = feat.reshape(feat.size(0), -1)
        out = self.classifier(feat)
        return out