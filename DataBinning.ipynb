{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This Notebook is used for the extraction and saving of binned data derived from raw data outputs. As formats of code for MNIST and CIFAR-10 where different, implementations for both have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function created by Bo: \n",
    "# https://github.com/lyn1874/region_based_active_learning/blob/main/eval_calibration/calibration_lib.py\n",
    "def bin_predictions_and_accuracies(probabilities, ground_truth, bins=10):\n",
    "    \"\"\"\n",
    "    Author: Bo Li, 2020\n",
    "    A helper function which histograms a vector of probabilities into bins.\n",
    "  \n",
    "    Args:\n",
    "      probabilities: A numpy vector of N probabilities assigned to each prediction\n",
    "      ground_truth: A numpy vector of N ground truth labels in {0,1}\n",
    "      bins: Number of equal width bins to bin predictions into in [0, 1], or an\n",
    "        array representing bin edges.\n",
    "  \n",
    "    Returns:\n",
    "      bin_edges: Numpy vector of floats containing the edges of the bins\n",
    "        (including leftmost and rightmost).\n",
    "      accuracies: Numpy vector of floats for the average accuracy of the\n",
    "        predictions in each bin.\n",
    "      counts: Numpy vector of ints containing the number of examples per bin.\n",
    "    \"\"\"\n",
    "    _validate_probabilities(probabilities)\n",
    "    _check_rank_nonempty(rank=1,\n",
    "                         probabilities=probabilities,\n",
    "                         ground_truth=ground_truth)\n",
    "\n",
    "    if len(probabilities) != len(ground_truth):\n",
    "        raise ValueError(\n",
    "            'Probabilies and ground truth must have the same number of elements.')\n",
    "\n",
    "    if [v for v in ground_truth if v not in [0., 1., True, False]]:\n",
    "        raise ValueError(\n",
    "            'Ground truth must contain binary labels {0,1} or {False, True}.')\n",
    "\n",
    "    if isinstance(bins, int):\n",
    "        num_bins = bins\n",
    "    else:\n",
    "        num_bins = bins.size - 1\n",
    "\n",
    "    # Ensure probabilities are never 0, since the bins in np.digitize are open on\n",
    "    # one side.\n",
    "    probabilities = np.where(probabilities == 0, 1e-8, probabilities)\n",
    "    counts, bin_edges = np.histogram(probabilities, bins=bins, range=[0., 1.])\n",
    "    indices = np.digitize(probabilities, bin_edges, right=True)\n",
    "    accuracies = np.array([np.mean(ground_truth[indices == i])\n",
    "                           for i in range(1, num_bins + 1)])\n",
    "    return bin_edges, accuracies, counts\n",
    "\n",
    "def _validate_probabilities(probabilities, multiclass=False):\n",
    "    if np.max(probabilities) > 1. or np.min(probabilities) < 0.:\n",
    "        raise ValueError('All probabilities must be in [0,1].')\n",
    "    if multiclass and not np.allclose(1, np.sum(probabilities, axis=-1),\n",
    "                                      atol=1e-5):\n",
    "        raise ValueError(\n",
    "            'Multiclass probabilities must sum to 1 along the last dimension.')\n",
    "\n",
    "def _check_rank_nonempty(rank, **kwargs):\n",
    "    for key, array in six.iteritems(kwargs):\n",
    "        if len(array) <= 1 or array.ndim != rank:\n",
    "            raise ValueError(\n",
    "                '%s must be a rank-1 array of length > 1; actual shape is %s.' %\n",
    "                (key, array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data specific attributes\n",
    "directory = \"./Data/MNIST-iid-5epochs/Version_00_iid/\"\n",
    "clients = 10\n",
    "classes = 10\n",
    "rounds = 91\n",
    "n_val = 10240\n",
    "nametag = \"iid-MNIST_\"\n",
    "\n",
    "# methodological attributes\n",
    "bins =40\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#Preallocating arrays\n",
    "overall_accuracy = np.zeros(rounds)\n",
    "bin_accuracies = np.zeros((rounds,bins))\n",
    "bin_counts = np.zeros((rounds,bins))\n",
    "\n",
    "for r in range(0,rounds):\n",
    "    #Reading relevant files for current round\n",
    "    if r<10:\n",
    "        rdir = directory + \"Round_0\" + str(r) + '/'\n",
    "    else:\n",
    "        rdir = directory + \"Round_\" + str(r) + '/'\n",
    "\n",
    "    ground_truth = pd.read_csv(rdir + '_Ground_Truth_Indices.csv',header=0).to_numpy().squeeze()\n",
    "    prediction = pd.read_csv(rdir + '_Index_Matrix.csv',header=0).to_numpy().squeeze()\n",
    "    \n",
    "    #Calculating overall accuracy for current round\n",
    "    overall_accuracy[r] = np.mean(ground_truth == prediction)\n",
    "    prop = pd.read_csv(rdir + '_server_.csv',header=0).to_numpy()\n",
    "    prop = np.max(prop,axis=1)\n",
    "\n",
    "    #Binning\n",
    "    edges, bin_accuracies[r,:], bin_counts[r,:] = bin_predictions_and_accuracies(prop,  ground_truth == prediction, bins=bins)\n",
    "    \n",
    "np.save(nametag+'overall_accuracy',overall_accuracy)\n",
    "np.save(nametag+'accVunc.npy',bin_accuracies)\n",
    "np.save(nametag+'binCounts.npy',bin_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10\n",
    "We only outputted training accuracy for some experiments of CIFAR-10, thus option for getting training accuracy is there in those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data specific attributes\n",
    "directory = \"./Data/Cifar_VGG11/\"\n",
    "clients = 10\n",
    "classes = 10\n",
    "rounds = 81\n",
    "n_val = 10000\n",
    "nametag = \"VGG11_Cifar_\"\n",
    "\n",
    "# methodological attributes\n",
    "bins =40\n",
    "train_available = True\n",
    "\n",
    "#############################################################\n",
    "#Preallocating arrays\n",
    "overall_accuracy = np.zeros(rounds)\n",
    "train_accuracy = np.zeros(rounds)\n",
    "bin_accuracies = np.zeros((rounds,bins))\n",
    "bin_counts = np.zeros((rounds,bins))\n",
    "\n",
    "for r in range(0,rounds):\n",
    "    #Reading relevant files for current round\n",
    "    if r<10:\n",
    "        rdir = directory + \"communication_round_00\" + str(r) + '/'\n",
    "    else:\n",
    "        rdir = directory + \"communication_round_0\" + str(r) + '/'\n",
    "\n",
    "    taf = pd.read_csv(rdir + 'taf.csv',header=None).to_numpy()\n",
    "    ground_truth = taf[:,0].astype(int)\n",
    "    prediction = taf[:,1].astype(int)\n",
    "\n",
    "    #Calculating overall test accuracy for current round\n",
    "    overall_accuracy[r] = np.mean(ground_truth == prediction)\n",
    "\n",
    "    if train_available == True:\n",
    "        taf_train = pd.read_csv(rdir + 'taf_train.csv',header=None).to_numpy()\n",
    "        ttrue = taf_train[:,0].astype(int)\n",
    "        tpred = taf_train[:,1].astype(int)\n",
    "        train_accuracy[r] = np.mean(ttrue == tpred)\n",
    "    \n",
    "    prop = pd.read_csv(rdir + 'server_pred.csv',header=None).to_numpy()\n",
    "    prop = softmax(prop,axis=1)\n",
    "    prop = np.max(prop,axis=1)\n",
    "        \n",
    "    #Binning\n",
    "    edges, bin_accuracies[r,:], bin_counts[r,:] = bin_predictions_and_accuracies(prop,  ground_truth == prediction, bins=bins)\n",
    "\n",
    "np.save(nametag+'overall_accuracy',overall_accuracy)\n",
    "np.save(nametag+'accVunc.npy',bin_accuracies)\n",
    "np.save(nametag+'binCounts.npy',bin_counts)\n",
    "\n",
    "if train_available == True:\n",
    "    np.save(nametag+'train_accuracy',train_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
